{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:32:34.690662Z",
     "start_time": "2025-12-16T06:32:34.687821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ],
   "id": "b29500e82dae8710",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:32:38.385950Z",
     "start_time": "2025-12-16T06:32:34.756357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import cross_entropy, normalize, softmax\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "\n",
    "\n",
    "import faiss\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.catalog import FungiTasticCatalog, CUBCatalog, Catalog\n",
    "from src.data.datasets.image_dataset import ImageDataset\n",
    "\n",
    "from src.config import EMBEDDING_KEY_NAME, FILENAME_KEY_NAME, TRANSFORMATION_KEY_NAME, TARGET_KEY_NAME\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "id": "21eab78b9c909f1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:32:38.504979Z",
     "start_time": "2025-12-16T06:32:38.437473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "d9cca71c207afa61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define parameters",
   "id": "9d60a7ea6b75dde2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:32:39.953710Z",
     "start_time": "2025-12-16T06:32:39.924296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_ROOT = \"/media/marek/disk/datasets/MPV\"\n",
    "OUTPUT_DIR = \"../logs/embeddings\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ],
   "id": "8f4f626bb93f341",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load model",
   "id": "72fc7ff62ee26fcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:32:41.923847Z",
     "start_time": "2025-12-16T06:32:41.895082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_df_with_embeddings( embedding_path: str):\n",
    "    \"\"\"Load embeddings and metadata to recreate a FungiTastic dataset with precomputed features.\"\"\"\n",
    "    print(f\"Loading embeddings from {embedding_path}\")\n",
    "    saved_data = torch.load(embedding_path, map_location=\"cpu\",  weights_only=False)\n",
    "    emb_data = saved_data[\"embeddings_data\"]\n",
    "\n",
    "    emb_df = pd.DataFrame({\n",
    "        EMBEDDING_KEY_NAME: list(emb_data[EMBEDDING_KEY_NAME]),\n",
    "        TRANSFORMATION_KEY_NAME: emb_data[TRANSFORMATION_KEY_NAME],\n",
    "        TARGET_KEY_NAME: emb_data[TARGET_KEY_NAME],\n",
    "        FILENAME_KEY_NAME: emb_data[FILENAME_KEY_NAME],\n",
    "    })\n",
    "\n",
    "    return emb_df\n"
   ],
   "id": "da0ecfe58290ae17",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:22.786523Z",
     "start_time": "2025-12-16T06:33:19.171019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_sub_dir = osp.join(OUTPUT_DIR, \"MPV-FewShot\")\n",
    "load_file_names = [\n",
    "    \"facebook-dinov3-vit7b16-pretrain-lvd1689m_512\",\n",
    "    # \"BAAI-EVA-CLIP-18B_224\",\n",
    "    \"hf-hub_BVRA-swin_base_patch4_window12_384.in1k_ft_fungitastic_384_224\",\n",
    "    \"hf-hub_BVRA-vit_base_patch16_224.in1k_ft_fungitastic_224_224\",\n",
    "    # \"vit_pe_core_gigantic_patch14_448_448\"\n",
    "]\n",
    "\n",
    "shortcuts = [\n",
    "    \"dinov3\",\n",
    "    # \"evaclip\",\n",
    "    \"bvra-swin\",\n",
    "    \"bvra-vit\",\n",
    "    # \"pe_core\"\n",
    "]\n",
    "\n",
    "embedding_dfs = {}\n",
    "for shortcut, load_file_name in zip(shortcuts, load_file_names):\n",
    "    try:\n",
    "        train_emb = load_df_with_embeddings(embedding_path=osp.join(output_sub_dir, f\"train_{load_file_name}.pth\"))\n",
    "        val_emb = load_df_with_embeddings(embedding_path=osp.join(output_sub_dir, f\"val_{load_file_name}.pth\"))\n",
    "        test_emb = load_df_with_embeddings(embedding_path=osp.join(output_sub_dir, f\"test_{load_file_name}.pth\"))\n",
    "\n",
    "        embedding_df = pd.concat([train_emb, val_emb, test_emb])\n",
    "        # embedding_df = pd.concat([val_emb])\n",
    "        embedding_dfs[shortcut] = embedding_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "embedding_dimensions = {key: df[\"embedding\"].iloc[0].shape[0] for key, df in embedding_dfs.items()}\n"
   ],
   "id": "e9b3b0f3f4faf0b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/train_facebook-dinov3-vit7b16-pretrain-lvd1689m_512.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/val_facebook-dinov3-vit7b16-pretrain-lvd1689m_512.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/test_facebook-dinov3-vit7b16-pretrain-lvd1689m_512.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/train_hf-hub_BVRA-swin_base_patch4_window12_384.in1k_ft_fungitastic_384_224.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/val_hf-hub_BVRA-swin_base_patch4_window12_384.in1k_ft_fungitastic_384_224.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/test_hf-hub_BVRA-swin_base_patch4_window12_384.in1k_ft_fungitastic_384_224.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/train_hf-hub_BVRA-vit_base_patch16_224.in1k_ft_fungitastic_224_224.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/val_hf-hub_BVRA-vit_base_patch16_224.in1k_ft_fungitastic_224_224.pth\n",
      "Loading embeddings from ../logs/embeddings/MPV-FewShot/test_hf-hub_BVRA-vit_base_patch16_224.in1k_ft_fungitastic_224_224.pth\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:27.516480Z",
     "start_time": "2025-12-16T06:33:25.587387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "for sc in shortcuts:\n",
    "    embedding_dfs[sc] = embedding_dfs[sc].rename(columns={\"embedding\": f\"embedding_{sc}\"})\n",
    "\n",
    "# 2. Start with the first DF\n",
    "merged = embedding_dfs[shortcuts[0]][[\"filename\", \"transform\", f\"embedding_{shortcuts[0]}\"]]\n",
    "\n",
    "# 3. Merge the rest iteratively using only necessary columns\n",
    "for sc in shortcuts[1:]:\n",
    "    df = embedding_dfs[sc][[\"filename\", \"transform\", f\"embedding_{sc}\"]]\n",
    "    merged = pd.merge(\n",
    "        merged,\n",
    "        df,\n",
    "        on=[\"filename\", \"transform\"],\n",
    "        how=\"inner\"  # keep only matching filename + transform\n",
    "    )\n",
    "    # Optional: free memory of df after merge\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "# 4. Concatenate embeddings row-wise without apply\n",
    "embedding_cols = [f\"embedding_{sc}\" for sc in shortcuts]\n",
    "\n",
    "# Preallocate an array for all concatenated embeddings\n",
    "concat_embs = np.empty((len(merged), sum(embedding_dfs[sc][f\"embedding_{sc}\"].iloc[0].shape[0] for sc in shortcuts)), dtype=np.float32)\n",
    "\n",
    "start = 0\n",
    "for sc in shortcuts:\n",
    "    col_embs = np.stack(merged[f\"embedding_{sc}\"].values).astype(np.float32)\n",
    "    end = start + col_embs.shape[1]\n",
    "    concat_embs[:, start:end] = col_embs\n",
    "    start = end\n",
    "    # free memory\n",
    "    merged.drop(columns=[f\"embedding_{sc}\"], inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "merged[\"embedding\"] = list(concat_embs)  # final concatenated embedding\n"
   ],
   "id": "94514fb907210626",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:28.387465Z",
     "start_time": "2025-12-16T06:33:28.180455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "catalog = FungiTasticCatalog(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    dataset_variant=\"fewshot\",\n",
    "    dataset_size=\"720p\",\n",
    "    download=False,\n",
    "    keep_zip=False,\n",
    ")\n",
    "\n",
    "catalog.add_embeddings(merged, validate=\"one_to_many\")\n",
    "df = catalog.get_metadata()\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "val_df = df[df[\"split\"] == \"val\"]\n",
    "test_df = df[df[\"split\"] == \"test\"]\n"
   ],
   "id": "1760bf88ab71f06b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:46.727899Z",
     "start_time": "2025-12-16T06:33:46.460096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del merged\n",
    "del embedding_dfs\n",
    "gc.collect()"
   ],
   "id": "a9534d7bba34aabc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# FungiCLEF 2025\n",
    "https://ceur-ws.org/Vol-4038/paper_239.pdf"
   ],
   "id": "1893d051db7a621f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:48.455610Z",
     "start_time": "2025-12-16T06:33:48.425221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.embeddings = dataframe[EMBEDDING_KEY_NAME].values\n",
    "        self.labels = dataframe[TARGET_KEY_NAME].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32).squeeze()\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return emb, label"
   ],
   "id": "a4bf26e878d31cd6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:51.046768Z",
     "start_time": "2025-12-16T06:33:51.010986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProjectionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedder_dims, projection_dim=512, use_layernorm=False, use_dropout=False, dropout_rate=0.1,\n",
    "                 internal_dim=1024, use_attention=False, attention_dim=512, extra_layer=False):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "        self.num_embedders = len(embedder_dims) if use_attention else None\n",
    "        self.embedder_dims = embedder_dims if use_attention else None\n",
    "        self.fixed_dim = attention_dim\n",
    "\n",
    "        if use_attention:\n",
    "            self.attn_weights = torch.nn.Parameter(torch.ones(self.num_embedders))\n",
    "            # ensures we can stack the embeddings\n",
    "            self.attn_projections = torch.nn.ModuleList([\n",
    "                torch.nn.Sequential(torch.nn.Linear(emb_dim, self.fixed_dim), torch.nn.ReLU()) for emb_dim in embedder_dims\n",
    "            ])\n",
    "\n",
    "        layers = [\n",
    "            torch.nn.Linear(self.fixed_dim if use_attention else input_dim, internal_dim * 2 if extra_layer else internal_dim),\n",
    "            torch.nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        if use_layernorm:\n",
    "            layers.append(torch.nn.LayerNorm(internal_dim))\n",
    "        if use_dropout:\n",
    "            layers.append(torch.nn.Dropout(dropout_rate))\n",
    "\n",
    "        if extra_layer:\n",
    "            layers.extend([torch.nn.Linear(internal_dim * 2, internal_dim), torch.nn.ReLU()])\n",
    "\n",
    "        layers.append(torch.nn.Linear(internal_dim, projection_dim))\n",
    "\n",
    "        if use_layernorm:\n",
    "            layers.append(torch.nn.LayerNorm(projection_dim))\n",
    "\n",
    "        self.projection = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_attention:\n",
    "            if x.dim() == 3 and x.shape[1] == 1:\n",
    "                x = x.squeeze(1)  # Squeeze out the second dimension\n",
    "            # split the embeddings out\n",
    "            start = 0\n",
    "            embeddings = []\n",
    "            for emb_dim, proj_layer in zip(self.embedder_dims, self.attn_projections):\n",
    "                embedding = x[:, start:start + emb_dim]\n",
    "                projected_embedding = proj_layer(embedding)\n",
    "                embeddings.append(projected_embedding)\n",
    "                start += emb_dim\n",
    "            weights = softmax(self.attn_weights, dim=0)\n",
    "            # for embed in embeddings:\n",
    "            #     print(embed.shape)\n",
    "            x = torch.stack([w * e for w, e in zip(weights, embeddings)], dim=0).sum(dim=0)\n",
    "            # print(\"after attention projections:\", x.shape)\n",
    "            x = self.projection(x)\n",
    "            # print(\"after final projection:\", x.shape)\n",
    "        else:\n",
    "            x = self.projection(normalize(x, p=2, dim=-1))\n",
    "        # x = self.projection(x)\n",
    "        return normalize(x, p=2, dim=-1)\n",
    "\n",
    "\n",
    "class CosineClassifier(torch.nn.Module):\n",
    "    \"\"\"Classifier to train the ProjectionModel\"\"\"\n",
    "    def __init__(self, embed_dim, num_classes, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(num_classes, embed_dim))\n",
    "        self.scale = scale  # Optional learnable scaling\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, D]\n",
    "        x = normalize(x, p=2, dim=-1)\n",
    "        w = normalize(self.weight, p=2, dim=-1)\n",
    "        return self.scale * torch.matmul(x, w.T)"
   ],
   "id": "6282133ffa16dc96",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:54.635702Z",
     "start_time": "2025-12-16T06:33:54.602251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MultiEmbedderProjection(torch.nn.Module):\n",
    "    def __init__(self, embed_dims, projection_dim=512, hidden_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embedders = len(embed_dims)\n",
    "        hidden_dim = hidden_dim or projection_dim\n",
    "\n",
    "        # 1) per-embedder projection to common space\n",
    "        self.per_embedder = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(d, hidden_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.LayerNorm(hidden_dim)\n",
    "            )\n",
    "            for d in embed_dims\n",
    "        ])\n",
    "\n",
    "        # 2) learnable scalar gates\n",
    "        self.gates = torch.nn.Parameter(torch.zeros(self.num_embedders))\n",
    "\n",
    "        # 3) final projection to desired embedding dim\n",
    "        self.project = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, projection_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(projection_dim, projection_dim),\n",
    "            torch.nn.LayerNorm(projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is concatenated embeddings of shape [B, sum(embed_dims)]\n",
    "        \"\"\"\n",
    "        # Split into per-embedder chunks\n",
    "        chunks = torch.split(x, [p[0].in_features for p in self.per_embedder], dim=-1)\n",
    "\n",
    "        # Per-embedder projected embeddings\n",
    "        projected = [proj(chunk) for proj, chunk in zip(self.per_embedder, chunks)]\n",
    "\n",
    "        # Soft gating\n",
    "        weights = F.softmax(self.gates, dim=0)\n",
    "\n",
    "        fused = sum(w * h for w, h in zip(weights, projected))\n",
    "\n",
    "        # Final CLIP-style projection\n",
    "        out = self.project(fused)\n",
    "\n",
    "        return F.normalize(out, dim=-1)\n",
    "\n",
    "\n",
    "class FewShotProjection(torch.nn.Module):\n",
    "    def __init__(self, embed_dims, projection_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embedders = len(embed_dims)\n",
    "\n",
    "        # 1) Per-embedder lightweight adapters\n",
    "        self.per_embedder = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(d, projection_dim, bias=False),\n",
    "                torch.nn.LayerNorm(projection_dim)\n",
    "            )\n",
    "            for d in embed_dims\n",
    "        ])\n",
    "\n",
    "        # 2) Learnable gate for each embedder (scalar)\n",
    "        self.gates = torch.nn.Parameter(torch.zeros(self.num_embedders))\n",
    "\n",
    "        # 3) Final normalization (no deep MLP)\n",
    "        self.norm = torch.nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split input back into chunks per embedder\n",
    "        chunks = torch.split(x, [p[0].in_features for p in self.per_embedder], dim=-1)\n",
    "\n",
    "        # project each embedder\n",
    "        projected = [proj(chunk) for proj, chunk in zip(self.per_embedder, chunks)]\n",
    "\n",
    "        # soft fusion\n",
    "        weights = F.softmax(self.gates, dim=0)\n",
    "        fused = sum(w * h for w, h in zip(weights, projected))\n",
    "\n",
    "        # final normalized embedding\n",
    "        return F.normalize(self.norm(fused), dim=-1)\n"
   ],
   "id": "c3d4fae560e2da42",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:33:57.625950Z",
     "start_time": "2025-12-16T06:33:57.583694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.criterion.classification import SeesawLossWithLogits\n",
    "\n",
    "\n",
    "def train(model, classifier, train_loader, val_loader, num_epochs=300, patience=5, lr=1e-5, device='cuda'):\n",
    "    return _train_ce_infonce(model, classifier, train_loader, val_loader, num_epochs, patience, lr, device)\n",
    "\n",
    "class LearnableLossWeighting(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize log variances as learnable parameters\n",
    "        self.log_sigma_ce = torch.nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma_triplet = torch.nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, ce_loss, triplet_loss):\n",
    "        # From Kendall et al. CVPR 2018\n",
    "        loss = (\n",
    "                torch.exp(-self.log_sigma_ce) * ce_loss +\n",
    "                torch.exp(-self.log_sigma_triplet) * triplet_loss +\n",
    "                self.log_sigma_ce + self.log_sigma_triplet\n",
    "        )\n",
    "        return 0.5 * loss\n",
    "\n",
    "def _train_ce_infonce(\n",
    "    model,\n",
    "    classifier,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    patience,\n",
    "    lr,\n",
    "    device,\n",
    "    lambda_triplet=None\n",
    "):\n",
    "    model.to(device)\n",
    "    classifier.to(device)\n",
    "    if lambda_triplet == \"learned\" or lambda_triplet is None:\n",
    "        loss_weighter = LearnableLossWeighting().to(device)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list(model.parameters()) +\n",
    "            list(classifier.parameters()) +\n",
    "            list(loss_weighter.parameters()),\n",
    "            lr=lr, weight_decay=1e-4\n",
    "        )\n",
    "    elif isinstance(lambda_triplet, float) or isinstance(lambda_triplet, int):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list(model.parameters()) + list(classifier.parameters()),\n",
    "            lr=lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "    infonce_loss_func = NTXentLoss(temperature=0.07).to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_classifier_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    pbar = tqdm(range(num_epochs), total=num_epochs)\n",
    "\n",
    "    # loss_fn = SeesawLossWithLogits(df=train_loader.dataset.df).to(device)\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        classifier.train()\n",
    "        total_loss = 0.0\n",
    "        ce_loss_total = 0.0\n",
    "        triplet_loss_total = 0.0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            embeddings = model(x)\n",
    "            logits = classifier(embeddings)\n",
    "\n",
    "            # Cross-entropy\n",
    "            ce_loss = F.cross_entropy(logits, y)\n",
    "            # ce_loss = loss_fn(logits, y)\n",
    "\n",
    "            infonce_loss = infonce_loss_func(embeddings, y)\n",
    "\n",
    "\n",
    "            # Combined loss\n",
    "            if lambda_triplet == \"learned\" or lambda_triplet is None:\n",
    "                loss = loss_weighter(ce_loss, infonce_loss)\n",
    "            elif isinstance(lambda_triplet, float) or isinstance(lambda_triplet, int):\n",
    "                loss = ce_loss + lambda_triplet * infonce_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            ce_loss_total += ce_loss.item()\n",
    "            triplet_loss_total += infonce_loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                embeddings = model(x)\n",
    "                logits = classifier(embeddings)\n",
    "                ce_loss = F.cross_entropy(logits, y)\n",
    "\n",
    "                infonce_loss = infonce_loss_func(embeddings, y)\n",
    "\n",
    "                if lambda_triplet == \"learned\" or lambda_triplet is None:\n",
    "                    val_loss += (loss_weighter(ce_loss, infonce_loss)).item()\n",
    "                elif isinstance(lambda_triplet, float) or isinstance(lambda_triplet, int):\n",
    "                    val_loss += (ce_loss + lambda_triplet * infonce_loss).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        pbar.set_description(\n",
    "            f\"Epoch {epoch+1:3d} | \"\n",
    "            f\"Train CE: {ce_loss_total:.4f} | \"\n",
    "            f\"InfoNCE: {triplet_loss_total:.4f} | \"\n",
    "            f\"Total: {total_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_classifier_state = copy.deepcopy(classifier.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    classifier.load_state_dict(best_classifier_state)\n",
    "    return model, classifier"
   ],
   "id": "c0757fa6d94cfc08",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fit projection network",
   "id": "8f32619c94480998"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Top-5 Accuracy against validation set",
   "id": "b737bde8d885c977"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:34:02.410011Z",
     "start_time": "2025-12-16T06:34:02.368634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_embeddings_for_class(dataset: ImageDataset, cls):\n",
    "    df = dataset.get_dataset()\n",
    "    class_idxs = df[df[TARGET_KEY_NAME] == cls].index\n",
    "    return df.iloc[class_idxs][EMBEDDING_KEY_NAME]\n",
    "\n",
    "\n",
    "class PrototypeClassifier(torch.nn.Module):\n",
    "    def __init__(self, train_dataset, projection_model, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.train_dataset = train_dataset\n",
    "        self.projection_model = projection_model.to(self.device)\n",
    "        self.projection_model.eval()\n",
    "\n",
    "        self.n_classes, self.emb_dim = None, None\n",
    "\n",
    "        class_embeddings, _ = self._get_classifier_embeddings(train_dataset)\n",
    "\n",
    "        print(\"class embeddings shape before projection:\", class_embeddings[0].shape)\n",
    "        self.class_embeddings = [self.projection_model(class_embedding.to(device)) for class_embedding in class_embeddings]\n",
    "        print(\"class embeddings shape after projection:\", self.class_embeddings[0].shape)\n",
    "\n",
    "        self.class_prototypes = torch.nn.Parameter(self.get_mean_prototypes(self.class_embeddings), requires_grad=False)\n",
    "\n",
    "        print(\"prototypes shape:\", self.class_prototypes.shape)\n",
    "\n",
    "    def _get_classifier_embeddings(self, dataset_train: ImageDataset):\n",
    "        class_embeddings = []\n",
    "        empty_classes = []\n",
    "        self.n_classes = dataset_train.get_dataset()[TARGET_KEY_NAME].nunique()\n",
    "        for cls in range(self.n_classes):\n",
    "            cls_embs = get_embeddings_for_class(dataset_train, cls)\n",
    "            if len(cls_embs) == 0:\n",
    "                # if no embeddings for class, use zeros\n",
    "                empty_classes.append(cls)\n",
    "                class_embeddings.append(torch.zeros(1, dataset_train.emb_dim))\n",
    "            else:\n",
    "                class_embeddings.append(torch.tensor(np.vstack(cls_embs.values)))\n",
    "        return class_embeddings, empty_classes\n",
    "\n",
    "    def get_mean_prototypes(self, embeddings):\n",
    "        # return normalize(torch.stack([class_embs.mean(dim=0) for class_embs in embeddings]), p=2, dim=-1)\n",
    "        return torch.stack([class_embs.mean(dim=0) for class_embs in embeddings])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def make_prediction(self, embeddings, batch_size=2048):\n",
    "        probas_list = []\n",
    "\n",
    "        for i in range(0, len(embeddings), batch_size):\n",
    "            batch = embeddings[i:i+batch_size].to(self.device)\n",
    "            batch = self.projection_model(batch)\n",
    "            batch = F.normalize(batch, dim=-1)\n",
    "\n",
    "            sims = batch @ self.class_prototypes.T     # [B, C]\n",
    "            probas = F.softmax(sims, dim=1).cpu()\n",
    "\n",
    "            probas_list.append(probas)\n",
    "\n",
    "        return torch.cat(probas_list, dim=0)\n"
   ],
   "id": "bc714b6ee56f6c0a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:34:05.123659Z",
     "start_time": "2025-12-16T06:34:05.092550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_val_probas(train_dataset, eval_dataset, format_for_submission=False, batch_size=10, reduction=\"mean\", projection_model=None):\n",
    "\n",
    "    proba_accumulator = {}\n",
    "\n",
    "    query_embeddings = None\n",
    "\n",
    "    if projection_model is None:\n",
    "        projection_model = fitted_embedding_projection_model\n",
    "\n",
    "    classifier = PrototypeClassifier(train_dataset, projection_model=projection_model, device='cpu')\n",
    "\n",
    "    print(f\"Class prototypes shape: {classifier.class_prototypes.shape}\")\n",
    "\n",
    "    # Initialize a dictionary to store predictions\n",
    "    predictions = {}\n",
    "\n",
    "    # Process in batches\n",
    "    unique_observation_ids = eval_dataset.df[\"observationID\"].unique()\n",
    "    for i in range(0, len(unique_observation_ids), batch_size):\n",
    "        batch_ids = unique_observation_ids[i:i+batch_size]\n",
    "        batch_data = eval_dataset.df[eval_dataset.df[\"observationID\"].isin(batch_ids)]\n",
    "\n",
    "        # Process each filename group in this batch\n",
    "        batch_embeddings = []\n",
    "        batch_filenames = []\n",
    "\n",
    "        for filename, group in batch_data.groupby(\"observationID\", sort=False):\n",
    "            embeddings_array = group[EMBEDDING_KEY_NAME].to_numpy()\n",
    "            embeddings_array = np.vstack(embeddings_array).squeeze()\n",
    "            if reduction == \"median\":\n",
    "                avg_embedding = np.median(embeddings_array, axis=0, keepdims=True).squeeze()\n",
    "            elif reduction == \"mean\":\n",
    "                avg_embedding = np.mean(embeddings_array, axis=0, keepdims=True).squeeze()\n",
    "\n",
    "            batch_embeddings.append(avg_embedding)\n",
    "            batch_filenames.append(filename)\n",
    "\n",
    "        # Convert to tensor and make predictions\n",
    "        avg_embeddings = torch.tensor(np.array(batch_embeddings), dtype=torch.float32)\n",
    "        probas = classifier.make_prediction(avg_embeddings)\n",
    "\n",
    "        # # Add to predictions dictionary\n",
    "        # for fname, pred in zip(batch_filenames, top_5.indices.numpy()):\n",
    "        #     predictions[fname] = pred\n",
    "\n",
    "        for fname, proba in zip(batch_filenames, probas):\n",
    "            proba_accumulator[fname] = proba.clone()\n",
    "\n",
    "        # Free memory\n",
    "        del avg_embeddings, batch_embeddings, batch_data\n",
    "\n",
    "    return proba_accumulator"
   ],
   "id": "10e82ac2ee570eb8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:34:07.849298Z",
     "start_time": "2025-12-16T06:34:07.819293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def get_submission_from_summed_probas(eval_dataset, proba_accumulator, format_for_submission=False, k=5):\n",
    "    # Final prediction dictionary\n",
    "    final_predictions = {}\n",
    "\n",
    "    # For each observation, get top-5 from accumulated probabilities\n",
    "    for fname, proba in proba_accumulator.items():\n",
    "        top5 = torch.topk(proba, k=k)\n",
    "        final_predictions[fname] = top5.indices.numpy()  # or top5.values if needed too\n",
    "\n",
    "    # Map predictions back to eval dataset\n",
    "    eval_dataset.df[\"preds\"] = eval_dataset.df[\"observationID\"].map(final_predictions)\n",
    "\n",
    "    submission = eval_dataset.df.copy()\n",
    "    submission = submission.drop_duplicates(subset=\"observationID\")\n",
    "\n",
    "    if format_for_submission:\n",
    "        submission = submission[[\"observationID\", \"preds\"]]\n",
    "        submission['preds'] = submission['preds'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "    return submission"
   ],
   "id": "432c93653dfc91e6",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fit the projection model and evaluate the validation top5 accuracy",
   "id": "36fe8d78d71e1686"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:34:16.002496Z",
     "start_time": "2025-12-16T06:34:15.971992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def split_probability(n_obs, max_prob=0.9, steepness=1.0, midpoint=5):\n",
    "    \"\"\"Probabilistic curve for splitting based on number of observations.\"\"\"\n",
    "    return max_prob / (1 + math.exp(-steepness * (n_obs - midpoint)))\n",
    "\n",
    "def probabilistic_train_val_split(\n",
    "        df: pd.DataFrame,\n",
    "        class_col: str = TARGET_KEY_NAME,\n",
    "        obs_col: str = \"observationID\",\n",
    "        val_frac: float = 0.2,\n",
    "        random_state: int | None = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    train_idx, val_idx = [], []\n",
    "\n",
    "    for cls, g in df.groupby(class_col):\n",
    "        obs_ids = g[obs_col].unique()\n",
    "        n_obs = len(obs_ids)\n",
    "\n",
    "        if n_obs == 1:\n",
    "            train_idx.extend(g.index)\n",
    "            continue\n",
    "\n",
    "        p_split = split_probability(n_obs)\n",
    "\n",
    "        if rng.random() > p_split:\n",
    "            train_idx.extend(g.index)\n",
    "            continue\n",
    "\n",
    "        n_val_obs = max(1, int(round(n_obs * val_frac)))\n",
    "        val_obs = rng.choice(obs_ids, size=n_val_obs, replace=False)\n",
    "\n",
    "        val_mask = g[obs_col].isin(val_obs)\n",
    "        val_idx.extend(g[val_mask].index)\n",
    "        train_idx.extend(g[~val_mask].index)\n",
    "\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    return train_df, val_df\n"
   ],
   "id": "c1be899661241f82",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# illustrate the split probability increasing with n_obs, 0.45 with midpoint observations, capping at 0.9\n",
    "for i in range(45):\n",
    "    print(i, split_probability(i))"
   ],
   "id": "da91b07083b65a51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:37:44.848879Z",
     "start_time": "2025-12-16T06:34:19.191431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "\n",
    "TRANSFORM = None\n",
    "train_dataset = ImageDataset(df=train_df, transform=TRANSFORM)\n",
    "val_dataset = ImageDataset(df=val_df, transform=TRANSFORM)\n",
    "test_dataset = ImageDataset(df=test_df, transform=TRANSFORM)\n",
    "\n",
    "input_dim = train_dataset.df[EMBEDDING_KEY_NAME].iloc[0].shape[-1]\n",
    "print(\"input dim before projection\", input_dim)\n",
    "num_classes = train_dataset.df[TARGET_KEY_NAME].nunique()\n",
    "print(\"num classes\", num_classes)\n",
    "projection_embedder_dimension = 1024 # 768\n",
    "print(\"projection embedder dim\", projection_embedder_dimension)\n",
    "embedder_dims = list(embedding_dimensions.values())\n",
    "print(\"embedder dims\", embedder_dims)\n",
    "\n",
    "train_val_df = pd.concat([train_dataset.df, val_dataset.df], ignore_index=True)\n",
    "train_df, val_df = probabilistic_train_val_split(\n",
    "    train_val_df,\n",
    "    class_col=TARGET_KEY_NAME,\n",
    "    obs_col=\"observationID\",\n",
    "    # obs_col=\"image_path\",\n",
    "    val_frac=0.1,\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(EmbeddingDataset(train_df), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(EmbeddingDataset(val_df), batch_size=batch_size)\n",
    "\n",
    "projection_model = ProjectionModel(\n",
    "    input_dim=input_dim, embedder_dims=embedder_dims, projection_dim=projection_embedder_dimension,\n",
    "   use_layernorm=False,\n",
    "   use_dropout=False, dropout_rate=0.25,\n",
    "   use_attention=False, attention_dim=512,\n",
    "   internal_dim=2048, extra_layer=False\n",
    ")\n",
    "\n",
    "classifier = CosineClassifier(embed_dim=projection_embedder_dimension, num_classes=num_classes)\n",
    "\n",
    "fitted_embedding_projection_model, fitted_classifier = train(projection_model, classifier, train_loader, val_loader, num_epochs=100)\n",
    "# submission = get_val_predictions(train_dataset, val_dataset, query_aware_prototypes=False, use_tim=use_tim)\n",
    "\n",
    "# probas_dict = get_val_probas(train_dataset, val_dataset, format_for_submission=False, batch_size=10,\n",
    "#                              reduction=\"mean\", projection_model=fitted_embedding_projection_model)\n",
    "#\n",
    "# # Create a defaultdict to accumulate summed probabilities\n",
    "# summed_probas = defaultdict(lambda: None)\n",
    "# # Loop over multiple models' probability outputs\n",
    "# for fname, proba in probas_dict.items():\n",
    "#     if summed_probas[fname] is None:\n",
    "#         summed_probas[fname] = proba.clone()\n",
    "#     else:\n",
    "#         summed_probas[fname] += proba\n",
    "#\n",
    "# submission = get_submission_from_summed_probas(val_dataset, summed_probas)\n",
    "#\n",
    "# labels = train_dataset.df.sort_values(TARGET_KEY_NAME)[TARGET_KEY_NAME].unique()\n",
    "# y_true = submission[TARGET_KEY_NAME].to_numpy()\n",
    "# y_pred = np.array(submission[\"preds\"].to_list())\n",
    "# y_pred = multi_hot_encode(y_pred, len(labels))\n",
    "# accuracy = top_k_accuracy_score(y_true, y_pred, k=5, labels=labels)\n",
    "# print(accuracy)\n"
   ],
   "id": "de17b1073fcac0fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim before projection 5888\n",
      "num classes 2427\n",
      "projection embedder dim 1024\n",
      "embedder dims [4096, 1024, 768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  33 | Train CE: 280.7676 | InfoNCE: 3.3047 | Total: 30.0815 | Val Loss: 1.6953:  32%|███▏      | 32/100 [03:25<07:16,  6.41s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(fitted_embedding_projection_model.state_dict(), \"./out/projection_model.pth\")\n",
    "torch.save(fitted_classifier.state_dict(), \"./out/tmp-classifier.pth\")"
   ],
   "id": "88b4f51c71e702c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:38:37.846358Z",
     "start_time": "2025-12-16T06:38:37.653773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "df2ca2741dec6c6a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Average the embeddings over the observationID and use that average embedding to make each classification",
   "id": "c5db469302a49ffe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T06:38:46.407794Z",
     "start_time": "2025-12-16T06:38:44.482512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_of_proba_dicts_final_submission = []\n",
    "train_val_dataset = ImageDataset(df=pd.concat([train_df, val_df]), transform=TRANSFORM)\n",
    "\n",
    "# Create a classifier as before\n",
    "classifier = PrototypeClassifier(train_val_dataset, projection_model=projection_model, device='cuda',)\n",
    "\n",
    "\n",
    "# Initialize lists to store averaged embeddings and filenames\n",
    "avg_embeddings_list = []\n",
    "filenames_list = []\n",
    "\n",
    "# Process each filename group\n",
    "for filename, group in test_dataset.df.groupby(\"observationID\", sort=False):\n",
    "    # Convert list of embeddings to array, making sure to squeeze extra dimensions\n",
    "    embeddings_array = group[EMBEDDING_KEY_NAME].to_numpy()\n",
    "\n",
    "    embeddings_array = np.vstack(embeddings_array).squeeze()\n",
    "    avg_embedding = np.mean(embeddings_array, axis=0, keepdims=True).squeeze()\n",
    "\n",
    "    avg_embeddings_list.append(avg_embedding)\n",
    "    filenames_list.append(filename)\n",
    "\n",
    "# Convert to tensor\n",
    "avg_embeddings = torch.tensor(np.array(avg_embeddings_list), dtype=torch.float32)\n",
    "print(f\"Averaged embeddings shape: {avg_embeddings.shape}\")"
   ],
   "id": "bfc9750ea5f552ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class embeddings shape before projection: torch.Size([44, 5888])\n",
      "class embeddings shape after projection: torch.Size([44, 1024])\n",
      "prototypes shape: torch.Size([2427, 1024])\n",
      "Averaged embeddings shape: torch.Size([999, 5888])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred_file_name = f\"./out/all_base-projection-1024-out_trainval_views_proto.csv\"\n",
    "\n",
    "# Make predictions using the averaged embeddings\n",
    "probas = classifier.make_prediction(avg_embeddings, batch_size=256)\n",
    "\n",
    "# Create a mapping from filenames to predictions\n",
    "probas_dict = {}\n",
    "for fname, proba in zip(filenames_list, probas):\n",
    "    probas_dict[fname] = proba.clone()\n",
    "\n",
    "list_of_proba_dicts_final_submission.append(probas_dict)\n",
    "\n",
    "# Create a defaultdict to accumulate summed probabilities\n",
    "summed_probas = defaultdict(lambda: None)\n",
    "# Loop over multiple models' probability outputs\n",
    "for model_probas in list_of_proba_dicts_final_submission:\n",
    "    for fname, proba in model_probas.items():\n",
    "        if summed_probas[fname] is None:\n",
    "            summed_probas[fname] = proba.clone()\n",
    "        else:\n",
    "            summed_probas[fname] += proba\n",
    "\n",
    "submission = get_submission_from_summed_probas(test_dataset, summed_probas, format_for_submission=True, k=10)\n",
    "\n",
    "submission = submission.rename(columns={\"observationID\": \"observationId\", \"preds\": \"predictions\"})\n",
    "submission['observationId'] = submission['observationId'].astype('Int64')\n",
    "submission.to_csv(pred_file_name, index=None)\n",
    "print(f\"saved submission file as {pred_file_name}\")\n",
    "\n",
    "display(submission.head())"
   ],
   "id": "be2afdbe9543151f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
